## Feedback on the Public Good Technology Rubric

### Our Approach to Feedback

Our researcher, Val Elefante, sought feedback on the rubric from **three people /organizations in the responsible technology space.** Feedback from Sam was delivered via email while feedback from John Willshire and Laure X Cast was delivered over Zoom calls. After having time to review the rubric, all three delivered unique feedback based on their specific background, expertise, and experience. This qualitative feedback has informed the proposed /suggested next steps that are detailed below.


### Executive Summary of the Feedback

**Major Themes from the Feedback:**

* Funders of responsible technology want some type of rubric. There is a clear desire for some methodology or structure that can be used to evaluate social impact technology tools.
* A need for examples / use cases/ best practices, possibly in a visual form (i.e. a map of projects evaluated at different maturity levels).
* Desire for collaboration with other nonprofits, funders, research organizations, and stakeholders in the responsible tech ecosystem (i.e. New Public, All Tech is Human, Metagov, and the Council for Tech and Social Cohesion)

**Ideas for Next Steps:**



* Conduct interviews with makers with rubric-based questions and begin documenting answers to start building the map/database of use cases / examples.
* Participate in ecosystem events and swap artifacts with other organizations building similar evaluation tools.


### More Detailed Summary of the Feedback

The section includes both verbatim and summarized feedback from the consultations conducted by our researcher, Val Elefante.


**Feedback from Sam at New_Public:**

* We emailed the Public Good Tech Rubric to [New_Public](https://newpublic.org/purpose), a nonprofit organization working towards building more flourishing digital public spaces. Sam from there had great feedback for the rubric which are [included in full below](#bookmark=kix.wfag3czhdp79). In summary here are his suggestions:
    * Include examples /use cases/ best practices (I’ve detailed how we could do this through a database and/or interview or blog post series below)
    * Collaborations with other interested parties (like them!). They offered to share it with their network and also opened the door for other collaboration opportunities.
        * He pointed me to their Civic Signals project: [https://newpublic.org/interactive](https://newpublic.org/interactive) 
        * As well as the [Social Spaces & Products Directory](https://airtable.com/appgA6QrWMpXmDp9X/shr7JFPTJt1C9j0qA/tblwlRhU0Icd8p5iJ) (What’s different: theirs is a focus on digital social spaces (social media), ours is more broad in types of tools we include).
    * Another piece of feedback he gave is that a few of the questions feel repetitive–namely the ones about “co-design /& feedback mechanisms” with communities. Personally, I am not bothered so much by this as I think it’s the secret sauce we’re trying to push. Plus, I think having these types of feedback at multiple stages of the technology design, development, and delivery process is key. But if there’s interest to consolidate, we certainly can.

**Feedback from John Willshire, Strategic Designer & Creator of the [Community Power Compass](https://smithery.com/2022/06/15/how-to-use-the-community-power-compass/)**

I emailed John for advice on how we might think about turning the rubric into some kind of interactive worksheet for makers to fill out (or other maps / exercises like that).
    * We talked about the pros and cons of self-reported vs. an external grading process (like TechSoup delivering scores). 
        * People don’t really get as much out of a self-assessment tool / there is no accountability mechanism
        * One avenue to take is for TechSoup to come up with its own evaluation system/ grading scale (numerical, pass/fail, etc.)
    * Alternatively, rather than one objective “grading system,” people and projects utilizing this rubric might want to know where, on some axes, they fall relative to other projects. It might make more sense to create a visual representation of where projects fall relative to each other based on the metrics established by this rubric.
        * One “scale” we might use is the maturity of a project - mapping projects on a timeline based on design phase and /or size of the project/its datasets. This way, makers can see tools that are further along than them on the journey and see decisions they made at each step of the way. 
        * Risk of public map:
            * Makers trying to manipulate to ensure they’re well-positioned on the relative map (or funders expecting something different from reality)
        * Visual inspiration:
        
![Visual Inspiration from Val](https://github.com/CaravanStudios/PublicGoodAppHouse/assets/3868907/caa5e89f-7a83-4ecb-99d4-a575d3199f43)
    
    * Other visual frameworks can be found at [Visual Frameworks: A language of patterns](https://visualframeworks.com/about/)
    
**Feedback from Laure X Cast, Prosocial Technology Researcher and Advocate**

(Prosocial Design, Integrity Institute, All Tech is Human, Council for Tech and Social Cohesion, etc.) 
    * She is working on planning an event for funders of prosocial tech for May 2024 in the Bay Area.
    * Saw a synthesis of Carol Sanford's work and Design Justice principles.
    * Ideal governance for public good technology = Nonprofit + decentralized (and/or federated?) + revenue-based + community & stakeholder governance “is kind of what feels right”
    * Laure has been speaking with funders of responsible technology and says “They want a rubric.” In fact, they have already started drafting up their own list of questions (See below) that they - and other potential funders - can use to evaluate technologies and if/how they have been designed for the public good.

**From Laure: Questions Funders of Responsible Technology Have for Makers:**

1. Are the incentives of users and product developers aligned?
2. Does the technology purport to be about people connection but actually is about media consumption?
3. Does the technology serve content driven by engagement-based algorithms?
4. Is there a sustainable business model (i.e. net positive within X number of years) without selling ads or user data?
5. What safety measures are in place to protect marginalized populations?
6. What safety measures are in place to prevent harassment?
7. What safety measures are in place to prevent unwanted sharing of location or other kinds of doxxing?
8. What safety measures are in place to protect children from bullying, harassment, or predators?
9. How does the platform deal with or have resilience to misinformation or deepfakes?
10. What are the environmental impacts of success for the platforms?
11. Who will be responsible for assessing harm? Who will be responsible for policies? What are the effects of the policies on society or communities?
12. How does the platform/product engage with regulation?
13. What is the governance model?
14. What is the staff makeup? How has the company invested in UX, CX, T&S? Who is responsible for what aspects of user experience? How does the company approach "good for its users" relative to "good for the business"?
15. What surveillance opportunities does this technology afford and how are companies considering the implications?
16. How many non-male or racialized people are part of the leadership of the company and part of the development team?
17. What is the growth strategy and how does it align to "delivering value for users" vs. exploiting users and their weaknesses?
18. What possible dark patterns are being employed in the service of growth?
19. Who might be negatively affected by the growth of this product?


## Ideas for Next Steps for the Rubric:

Our researcher, Val Elefante, proposes the following ideas as next steps for the Rubric:

* **Idea #1: Create a Visual (or Database) to start mapping and collecting projects:** 
    * I would suggest starting with an Interview / Q&A + Blog Post Series:** **we conduct an interview series with various different Makers of public good technology where the interview questions are pulled from the rubric questions. We can make a corresponding blog post series of these Q&As and allow people to browse and learn more that way. We could call it: “Evaluating Tools Using TechSoup’s Public Good Technology Rubric.” 
    * I think there is a specific type of nonprofit technologist that TechSoup has connections with that might be different from the usual suspects highlighted in “responsible tech” spaces–with special stories and meaningful experiences worth sharing further and wider. Not sure exactly who these people are but I sense they are among the usual TechSoup nonprofit operator crowd (John Romano of Disaster Central, Janastu, and others)
    * Then, we can populate a database / start building a map based on these interviews.
        * The Accelerating Makers[ DWeb for Impact Projects Database](https://www.notion.so/acceleratingmakers/e746d85728ed4f68aad731539cae1382?v=0b2e2afd3b4643f8afc205c6c23569c3&pvs=4) could be a great place for this–the one big difference is that this rubric is NOT DWeb exclusive (but the database is). [The template](https://www.notion.so/acceleratingmakers/Open-Referral-3b78dba5879745548da76b07e321592e?pvs=4) I created for entries has already been inspired partly by the questions of this rubric. We could add more / sync the projects more to make it more of two closely linked resources. If we do this, we should probably change the template so that it matches the sections of the rubric (though I worry about entries getting too long…?): I’ve extracted the IMO key questions we might pull out and also compared it to what’s already in the database.
      
<img width="908" alt="2024-05-21_10-01-32" src="https://github.com/CaravanStudios/PublicGoodAppHouse/assets/3868907/bb08d283-96cd-445c-b0f8-489400d69827">


* **Idea #3: Host conversations and participate in events**
    * If TechSoup does want to become a thought-leader in the responsible technology space, I would suggest getting involved in partnership with other organizations who are hosting events, facilitating conversations, and building tools (like the rubric!) to support the ecosystem - organizations include Metagov, Funding the Commons, New_Public, All Tech is Human, etc,. as well as various universities (Berkman Klein at Harvard, etc.)
    * Event ideas:
        * Bring funders together to discuss how they evaluate impactful technology.
        * Bring makers (grantees) together to discuss how they go about answering these questions and their challenges to making prosocial decisions.
    * Val is part of organizations who are already organizing these types of events, and would be able to easily help TechSoup plug in.
        * Most recently, Val (I) co-hosted the Web3 Grants Summit at ETH Denver where we facilitated events for both grantors and grantees. See the takeaways from those events: 
        *  


# APPENDIX


## Feedback from Sam from New_Public (in full):

1. It feels, in some ways, like a more universal version (not limited to digital social spaces) of what we set out to create with the initial Civic Signals&lt;[https://newpublic.org/interactive](https://newpublic.org/interactive)>; a POV on how things should be done, but this being slightly even more tactical/practical (in a good way!)

2. In whatever format this ends up taking, don't underestimate the part of the "About This Rubric" section that explains where this list came from - in particular, "This thinking is augmented with what we found was needed in interviews with the makers of digital solutions and what we have learned from quantitative research on the readiness of civil society organizations to adopt technology." (which could be expanded even more maybe over time). I feel like there are a bunch of these sorts of lists kicking around so the more this can be framed as a collective (vs. just a handful of people made it up one day), the more people will trust and use it, I think.

3. In its current state, it feels most useful for Makers as a sort of 'sense check' - a quick list one could run through and make sure they've accounted for all of these different factors. It feels most useful for the very beginning stage of an org/ tool/project development process. I can imagine, as i think you do too, a future evolution into some sort of sortable directory (similar to this directory&lt;[https://newpublic.org/directory](https://newpublic.org/directory)> we published last year, but not just limited to social tools as ours is, and likely more robust in assessment) w/ 'scores' or at least explanations/answers to each of the points in the rubric (ideally, externally validated in some way, not just self-reported) that'd be more useful for Funders and "Customers"/non-profits.

4. To make this more useful for Makers, I wonder if there might at some point be, for each of the bullet points in the rubric (e.g. in design, "How are stakeholders kept informed about the progress of tool development?" or "What is the current legal status / business structure of the project (C corp, B corp, LLC, nonprofit, etc.)?") more details / best practices / stories / case studies / or guides of how orgs building specific tools approached it. It's one thing to simply remind a builder 'oh, think about how you're keeping people informed about tool development' but even better to say: here's a model to replicate (i know this is no small task though lol). This might happen naturally though as different tools are assessed using the rubric.

5. I love the "Delivery" section - an important consideration that I agree is often overlooked

6. Small bit but there are a few places where bullets feel a little redundant, particularly around co-design /& feedback mechanisms — i.e. in Design: How is input collected from these stakeholders?, in Delivery: How can the community provide feedback on the tool experience and roadmap?, in Impact: Is there a structured way to make changes to the project as a result of these learnings? / Do users and community members have an opportunity to provide feedback on any evaluations?" and Community: "Do users of the tool have an opportunity to engage directly with people who are working on the technology?"

I noticed you mention the DPGA and poking around in there saw that they have this 'registry&lt;[https://app.digitalpublicgoods.net/a/10263](https://app.digitalpublicgoods.net/a/10263)>' which looks pretty cool. Am I interpreting right that you envision what you're building as slightly more expansive (not just limited to the UN's 17 SDGs)?

re: Next steps - there are prob a few different ways we might work together on this (whenever you feel the timing is right), ranging from us sharing it as a mention in our newsletter/socials, to more directly sharing w/ some builders in our network, to us using it ourselves for things we might build this year. Do you have a working timeline / plan for this over the next few months?
